% \documentclass[a4paper,10pt]{article}
\documentclass[twocolumn,a4paper,10pt]{article}
\usepackage{geometry}
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage{lmodern}
\usepackage[english]{babel}
\usepackage{url}
\usepackage{hyperref}
\usepackage{subcaption}
\usepackage{graphicx}
\usepackage{amsmath}

\geometry{a4paper,
 left=20mm,
 right=20mm,
 top=20mm,
 bottom=20mm,
}

% Training subcaption package to comply with
% IEEE standards. We can ignore the warning
% generated by caption.sty which is due to 
% the redefinition of \@makecaption
\DeclareCaptionLabelSeparator{periodspace}{.\quad}
\captionsetup{font=footnotesize,labelsep=periodspace,singlelinecheck=false}
\captionsetup[sub]{font=footnotesize,singlelinecheck=true}


\title{Machine Learning Homework 3:\\Unsupervised Learning and Dimensionality Reduction}
\author{Nicolas Six}



\makeatother

\begin{document}
\maketitle \tableofcontents{}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Introduction}
\paragraph{}

\subsection{Datasets} \label{intro_dataset}
\paragraph{}
For this work we focused on two different dataset. The first is the well known Iris dataset, this dataset has describe four different parameters of three different Iris variety and contain 151 different entries. We chose this dataset because of the low dimension space that it cover, allowing us to plot easily understandable 2D figures to show the results of clustering and dimension reduction algorithms. Figure \ref{fig:iris_org} show the repartition of the classes on the dataset. You can see that the classes are already well defined and we can so suppose that clustering algorithm will perform well.

\begin{figure}
  \centering
  \begin{subfigure}[t]{0.49\columnwidth} 
    \centering 
    \includegraphics[width=1.1\linewidth]{{../plotter/graphs/org_sepal_length_sepal_width_label}.png}
    \caption{Classes repartition according to sepal length and width}
    \label{fig:iris_org_sep}
  \end{subfigure}
  \begin{subfigure}[t]{0.49\columnwidth} 
    \centering 
    \includegraphics[width=1.1\linewidth]{{../plotter/graphs/org_petal_length_petal_width_label}.png}
    \caption{Classes repartition according to petal length and width}
    \label{fig:iris_org_pet}
  \end{subfigure}
  \caption{Repartition of the classes on Iris dataset according to different parameters}
  \label{fig:iris_org} 
\end{figure}

The second dataset is here again the player recognition dataset on Starcraft II. For a more in depth description please refer to Homework 1 and 2. This dataset is again very challenging here, as it as 200 classes visualization will be hard and it's 71 dimensions wont help to represent it on paper. In addition with only 3035 examples cluster are very small in average, with some with 4 elements while others have more than 50. As you can see on Figure \ref{fig:sc_org}, even when we select dimension which are recognized to easily differentiate players, such as the action per minutes (APM) or the race used, the plots are not readable. We also explored the possibility to add a third dimension, but it didn't improve the visualization here. In addition the different feature of this dataset are on very different intervals, going from 0 to 3 to 0 to 7000. As we know that such characteristic doesn't help clustering algorithm we choose to normalize all the different features.

\begin{figure}
  \centering
  \begin{subfigure}[t]{0.49\columnwidth} 
    \centering 
    \includegraphics[width=1.1\linewidth]{{../plotter/graphs/org_avg_micro_apm_max_ap5s_label}.png}
    \caption{Classes repartition according to avg APM and max number of action per 5 seconds}
    \label{fig:sc_org_apm}
  \end{subfigure}
  \begin{subfigure}[t]{0.49\columnwidth} 
    \centering 
    \includegraphics[width=1.1\linewidth]{{../plotter/graphs/org_race_avg_micro_apm_label}.png}
    \caption{Classes repartition according to avg APM and race used by the player}
    \label{fig:sc_org_race}
  \end{subfigure}
  \caption{Repartition of the classes on Starcraft dataset according to different parameters}
  \label{fig:sc_org} 
\end{figure}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Clustering}
\paragraph{}
Applying clustering like K-means or Expectation Maximization (EM) to our two different dataset give good results.

For the Iris dataset both clustering algorithm perform really well. As you can see on Figure \ref{fig:iris_kmeans}, kmeans get here a near perfect classification result. The typical potatoid shape of the kmeans clusters is easy to spot on Figure \ref{fig:iris_kmeans_pet}, but this particular bias of kmeans is also the reason why it make a few mistakes that are easy to spot on Figure \ref{fig:iris_kmeans_sep}. EM on it side perform even better than kmeans here. Few mistakes can be spotted on the edges between blue and white area of Figure \ref{fig:iris_EM_pet}, which is not surprising as the two original cluster were very close on every dimensions. In addtion we can note that given it's nature EM let more the blue and white cluster to mix together on Figure \ref{fig:iris_EM_sep}, this is due to the large space between point in the in this part of the cluster.

Such good results are not surprising for Iris dataset as it's really a toy dataset in terms of complexity, this was desired here to show how the algorithm behave.

\begin{figure}
  \centering
  \begin{subfigure}[t]{0.49\columnwidth} 
    \centering 
    \includegraphics[width=1.1\linewidth]{{../plotter/graphs/kmeans_sepal_length_sepal_width_label}.png}
    \caption{Classes repartition according to sepal length and width}
    \label{fig:iris_kmeans_sep}
  \end{subfigure}
  \begin{subfigure}[t]{0.49\columnwidth} 
    \centering 
    \includegraphics[width=1.1\linewidth]{{../plotter/graphs/kmeans_petal_length_petal_width_label}.png}
    \caption{Classes repartition according to petal length and width}
    \label{fig:iris_kmeans_pet}
  \end{subfigure}
  \caption{Repartition of the K-Means clustering results on Iris dataset according to different parameters}
  \label{fig:iris_kmeans} 
\end{figure}

\begin{figure}
  \centering
  \begin{subfigure}[t]{0.49\columnwidth} 
    \centering 
    \includegraphics[width=1.1\linewidth]{{../plotter/graphs/EM_sepal_length_sepal_width_label}.png}
    \caption{Classes repartition according to sepal length and width}
    \label{fig:iris_EM_sep}
  \end{subfigure}
  \begin{subfigure}[t]{0.49\columnwidth} 
    \centering 
    \includegraphics[width=1.1\linewidth]{{../plotter/graphs/EM_petal_length_petal_width_label}.png}
    \caption{Classes repartition according to petal length and width}
    \label{fig:iris_EM_pet}
  \end{subfigure}
  \caption{Repartition of the EM clustering results on Iris dataset according to different parameters}
  \label{fig:iris_EM} 
\end{figure}

The Starcraft dataset is tricky to represent and analyses for the same for the same reason as described in section \ref{intro_dataset}. However, if in general both clustering did not get good results we can note that the outliers are generally classify together, which is the right things to do according to Figure \ref{fig:sc_org}. This is easy to spot on Figures \ref{fig:sc_kmeans_sep} and \ref{fig:sc_EM_sep}. Such a comportment is not surprising as both clustering algorithms try to group similar features together, thus outliers are easy to group together while at the same time easy to spot when looking at the plots. The fact that those outliers and in the same class here allow us to suppose that they are close in others dimensions, as is they were not K-Means and EM will have chosen a different classes for each point or found link on other dimension to give result such as the one of Figure \ref{fig:iris_EM_sep}.

\begin{figure}
  \centering
  \begin{subfigure}[t]{0.49\columnwidth} 
    \centering 
    \includegraphics[width=1.1\linewidth]{{../plotter/graphs/kmeans_avg_micro_apm_max_ap5s_label}.png}
    \caption{Classes repartition according to avg APM and max number of action per 5 seconds}
    \label{fig:sc_kmeans_sep}
  \end{subfigure}
  \begin{subfigure}[t]{0.49\columnwidth} 
    \centering 
    \includegraphics[width=1.1\linewidth]{{../plotter/graphs/kmeans_race_avg_micro_apm_label}.png}
    \caption{Classes repartition according to avg APM and race used by the player}
    \label{fig:sc_kmeans_pet}
  \end{subfigure}
  \caption{Repartition of the K-Means clustering results on Starcraft dataset according to different parameters}
  \label{fig:sc_kmeans} 
\end{figure}

\begin{figure}
  \centering
  \begin{subfigure}[t]{0.49\columnwidth} 
    \centering 
    \includegraphics[width=1.1\linewidth]{{../plotter/graphs/EM_avg_micro_apm_max_ap5s_label}.png}
    \caption{Classes repartition according to avg APM and max number of action per 5 seconds}
    \label{fig:sc_EM_sep}
  \end{subfigure}
  \begin{subfigure}[t]{0.49\columnwidth} 
    \centering 
    \includegraphics[width=1.1\linewidth]{{../plotter/graphs/EM_race_avg_micro_apm_label}.png}
    \caption{Classes repartition according to avg APM and race used by the player}
    \label{fig:sc_EM_pet}
  \end{subfigure}
  \caption{Repartition of the EM clustering results on Starcraft dataset according to different parameters}
  \label{fig:sc_EM} 
\end{figure}

To the light of those results we can say that clustering algorithm are really dependent on the statistical repartition of the data. Even if data are normalized, density is not constant and induce bias in the clustering algorithm. An other bias came from the redundancy of the data. For Iris, it's not really the case as it has very few dimensions which were selected by an expert and have thus their part of domain knowledge. However for Starcraft, we can assume that a lot of information is redundant as no particular filter were applied, and correlation between features such as the average APM and the max APM on 5 seconds are trivial. This redundancy is equivalent to give more weight to some features, which can be very harmful to clustering if this features are not relevant. In the next section we will thus try different algorithms which do such things, without adding any domain knowledge.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Dimension reduction}
\paragraph{}
Reduce the number of dimension is often crucial in machine learning, particularly because the curse of dimensions. A lot of different algorithms exist to try to reduce the number of dimension to a set of meaningful one. But meaningful does not have any meaning on its own and it's probably why they are so many different ways to do it.

Here we will explore the results of the dimension reduction algorithms seen during the lessons: PCA, ICA, Random projections and LDA. The later is a bit particular as it make the projection knowing what you want to find with it, it's also why chose it, to show how giving such a crucial information change the results.

\subsection{PCA}
\paragraph{}
An interesting possibility with PCA is that you can easily score the dimensions chosen thanks to the associated eigenvalue. Plots representing those values for both dataset can be fund on Figure \ref{fig:pca_score}. On both case the eigenvalue follow an exponentially decreasing shape with the number of dimensions. Thus you can see that on Iris, according to PCA, one dimension already give a good description of the dataset while ten dimensions is still short to represent the Starcraft dataset.

\begin{figure}
  \centering
  \begin{subfigure}[t]{0.49\columnwidth} 
    \centering 
    \includegraphics[width=1.1\linewidth]{{../plotter/graphs/PCA_dimension_importance_4}.png}
    \caption{Iris dataset}
    \label{fig:pca_iris_score}
  \end{subfigure}
  \begin{subfigure}[t]{0.49\columnwidth} 
    \centering 
    \includegraphics[width=1.1\linewidth]{{../plotter/graphs/PCA_dimension_importance_70}.png}
    \caption{Starcraft dataset}
    \label{fig:pca_sc_score}
  \end{subfigure}
  \caption{Evolution of the score according to the number of dimension for both dataset}
  \label{fig:pca_score} 
\end{figure}

\begin{figure}
  \centering
  \begin{subfigure}[t]{0.49\columnwidth} 
    \centering 
    \includegraphics[width=1.1\linewidth]{{../plotter/graphs/pca_iris_0_1_label}.png}
    \caption{Iris dataset}
    \label{fig:pca_iris}
  \end{subfigure}
  \begin{subfigure}[t]{0.49\columnwidth} 
    \centering 
    \includegraphics[width=1.1\linewidth]{{../plotter/graphs/pca_sc_0_1_label}.png}
    \caption{Starcraft dataset}
    \label{fig:pca_sc}
  \end{subfigure}
  \caption{Representation of the classes after projection in the best plan fund by PCA}
  \label{fig:pca} 
\end{figure}

As you can see on Figure \ref{fig:pca}, the projections does it job. In the Iris case, the classes really appear on different clusters. We can also understand here why axes 0 as a better score than axes 1, as here the data nearly appear like group of vertical line, which is great given that PCA didn't the labels, but still managed to nearly split them on the first dimension.

For the Starcraft dataset the things are a bit different, if it's sure that the data are more widely spread in Figure \ref{fig:pca_sc} than it was before a PCA, the 200 classes still appear as noise. This is not surprising given that we try here to represent in two dimensions data that PCA say it can represent correctly in ten to twenty dimensions. It's hard to know if it's our brain that try to fit pattern or if they are really pattern here, but we can see that same colors are generally on similar area, which show that PCA find some consistent way to reduce the dimensionality, but that two dimensions wasn't enough.

\paragraph{}
In a clustering point of view, Iris is still an easy case. K-Means get here worst result than on the full dataset because it try to split the two cluster on the right in an horizontal way. This comportment can be countered by a different norm or data normalization, but doing so can be called supervised learning, which is not our goal here. EM get here near perfect results, probably taking more advantage of the hight density in the center of the clusters.

Starcraft suffer again from the lake of dimension, both clustering algorithm try to find cluster that do not exist. While this gave us beautiful plot of a large potatoid full of small colorful potatoid, we did not get back the players we were looking for. Which is not really surprising when you know how the players are split (Figure \ref{fig:pca_sc}).

\paragraph{}
In conclusion, PCA is fast and efficiently reduce the number of dimensions while giving us nice values to help us to chose how much dimension we want.

\subsection{ICA}
\paragraph{}
ICA look for independent underling variable and is so very used in signal processing when different signals are mixed together. In our case It's probably possible to fine underling variable to our data, the principal one being the label, but on our two cases it's hard to find other examples. This does not mean that ICA will not perform well here but that it's probably not the best algorithm we can use.

On the Iris dataset, as you can see on Figure \ref{fig:ica_iris}, the result are very similar than the ones of PCA (Figure \ref{fig:pca_iris}). If you perform a 180 degree rotation of the plot you will see that they are nearly identical. Thus, we can apply the same conclusion than for PCA here. But it's important to note that the scale are different, here the ratio of the range of axis 0 over the one of axis 1 is about 0.75, while it was 2.4 in PCA, explaining the clustering difference.

On Starcraft, The potatoid shape is quite different from PCA but it is not enough to split the different players apart, as you can see on Figure \ref{fig:ica_sc}. Still the result look better than for PCA, as color spot are easier to find.

\begin{figure}
  \centering
  \begin{subfigure}[t]{0.49\columnwidth} 
    \centering 
    \includegraphics[width=1.1\linewidth]{{../plotter/graphs/ica_iris_0_1_label}.png}
    \caption{Iris dataset}
    \label{fig:ica_iris}
  \end{subfigure}
  \begin{subfigure}[t]{0.49\columnwidth} 
    \centering 
    \includegraphics[width=1.1\linewidth]{{../plotter/graphs/ica_sc_0_1_label}.png}
    \caption{Starcraft dataset}
    \label{fig:ica_sc}
  \end{subfigure}
  \caption{Representation of the classes after projection in the best plan fund by ICA}
  \label{fig:ica} 
\end{figure}

\paragraph{}
On a clustering point of view, due to change of proportion both K-Means and EM wrongly split the left cluster and preferred to split them horizontally. In addition K-means managed to get wrong one point of the right cluster. As described for PCA, axes 0 is here the one carrying the more information, reducing it's amplitude regarding to the one of axes 1, give far more importance to to this last axis in both clustering experiment. Again one may say that this problem is easy to overcome, but doing it is not our goal here is tend to be supervised learning with its own problem such as overfiting.

Starcraft is still difficult here for clustering algorithms. The result are very similar to the ones of PCA. As the classes are really mixed together, clustering only manage to group some sort of similar player but didn't manage to find the real player.

\paragraph{}
In conclusion, ICA gave us very similar results than PCA, probably because our data were too difficult or too easy to differentiate the two algorithms, but also because none of the datasets has trivial underling variables.

\subsection{Randomized projections}
\paragraph{}
Random projection is a very time efficient way to reduce dimension. The fact that it is random driven let us expect worst results than the other algorithms explored here.

On Iris most of the random projection we tried gave average results, not better than a plan in the original nor really worst. As you can see on Figure \ref{fig:rnd_iris}, the global shape is familiar as very close to the one get for PCA and ICA, with one cluster far from the two others which are really close but with very few common part. This projection is far from optimal in a clustering point of view, but the cluster are still there.

The results on Starcraft are more interesting. Not that the player are well grouped together, but this projection managed to mix feature in a way that allow out-layers to stand out of the global mass. Thus we cannot expect clustering algorithm to get every classes right but they may fund so of them, which were not rely the case of both PCA and ICA.

\begin{figure}
  \centering
  \begin{subfigure}[t]{0.49\columnwidth} 
    \centering 
    \includegraphics[width=1.1\linewidth]{{../plotter/graphs/rnd_iris_0_1_label}.png}
    \caption{Iris dataset}
    \label{fig:rnd_iris}
  \end{subfigure}
  \begin{subfigure}[t]{0.49\columnwidth} 
    \centering 
    \includegraphics[width=1.1\linewidth]{{../plotter/graphs/rnd_sc_0_1_label}.png}
    \caption{Starcraft dataset}
    \label{fig:rnd_sc}
  \end{subfigure}
  \caption{Representation of the classes after projection in a plan by random projection}
  \label{fig:rnd} 
\end{figure}

\paragraph{}
Clustering on random projection mostly gave bad results on Iris. For example EM chose here to group the two bottom cluster together and to split the top one. While K-Means cut the bottom group in two with an horizontal line, one of the worst chose if you want to find the original labels.

On Starcraft, the clustering algorithm unsurprisingly gave bad results. Some outliers were correctly classified but were also most of the time split in multiple clusters. The central mass was as previously too diffuse to get any thing right out of it. As previously we can say that 2 dimensions are not enough to represent those data. Particularly with random projection, which may need more dimensions to conserve the same amount of information than PCA and ICA.

\paragraph{}
Thus, random projection does not do a great job to apply clustering algorithm, but still keep the global shape of the data and can be useful for quick visualization purpose or as reference to evaluate the performance of other dimension reduction algorithms.

\subsection{LDA}
\paragraph{}

LDA is very different from the others dimension reduction algorithm explored in this report is it's the only one to include the labels to make the best projection. This information make the whole clustering more supervised than other thing. Thus, LDA does not give the same information than other algorithms, as this one is biased toward what we are looking for. This bias is interesting here to see how this information change the results, but usually clustering is used when you do not know what you are looking for, case where LDA is useless.

The projection of Iris into two dimensions displayed on Figure \ref{fig:lda_iris} show particularly well split clusters. Two classes are still very close to each other, but the original data want that. We can, here, also look at the range ratio, but it will not be useful as axes 0 already give most of the information needed to correctly classify every thing, as long as this ration stay near 1, we can expect good clustering results later. So LDA project here the Iris dataset into nearly one dimension, showing that with only a linear combination of the variable we have enough information to find all the classes of this dataset.

The best projection fund by LDA for Starcraft is a bit different in term of shape from the one seen previously. Me must also note than players seems to be more close to each others than before, but players are still mixed together. So even when including what we are looking for, we are not able to linearly combine dimensions to project data to a plan where clustering algorithms will be able to manage them. Knowing that it is possible to get 90\% validation accuracy on this problem we can deduce then ether we need more dimension or linear combination is not sufficient here.

\begin{figure}
  \centering
  \begin{subfigure}[t]{0.49\columnwidth} 
    \centering 
    \includegraphics[width=1.1\linewidth]{{../plotter/graphs/lda_iris_0_1_label}.png}
    \caption{Iris dataset}
    \label{fig:lda_iris}
  \end{subfigure}
  \begin{subfigure}[t]{0.49\columnwidth} 
    \centering 
    \includegraphics[width=1.1\linewidth]{{../plotter/graphs/lda_sc_0_1_label}.png}
    \caption{Starcraft dataset}
    \label{fig:lda_sc}
  \end{subfigure}
  \caption{Representation of the classes after projection in the best plan fund by LDA}
  \label{fig:lda} 
\end{figure}

\paragraph{}
As already stated for Iris, the clustering after this projection is stretch forward. Both EM and K-Means does near perfect classification here in terms of original label. Both of them with some errors in the frontier of the two clusters, but in a smaller way than what we have seen before. They do not get confused by the second dimension probably because it range on a smaller interval than the first one.

For Starcraft, it's difficult to say if the result are better or worse than before. We can probably say that they are as worse as in all the previous cases. If you can see on Figure \ref{fig:lda_sc}, that globally all the games of a players tend to be on the same region, are still too mixed with other players to allow clustering algorithm to find them.

\paragraph{}
In conclusion LDA does a great job on Iris but doesn't do miracles on Starcraft. In addition for this little improvement with regard other algorithm such as PCA and ICA, LDA come with a large cost, the one of the labels. It is so not used in the same cases than the previous ones. While the tree others algorithms are very useful when you look for something in the data but not knowing exactly what, LDA is useful when you know what you are looking for and just want to reduce the dimensionality before running classification algorithm for example.

\subsection{Conclusion}
%execution time of EM on starcraft + Iris
% +3D
\paragraph{}
In this section we have seen that applying dimension reduction before clustering can be very efficient to compress the data while keeping them as meaningful as possible. This was really useful here for visualization purpose, which is not negligible when the main question is describe what you see, but can also be very useful to limit the effect of the curse of dimensionality.

Run time were not really discussed before, mainly because most of them are not significant. All the dimension reduction algorithm does there work in less than a few seconds, it's also the same for K-means, but EM is far slower taking about a minute to complete all the 30 runs asked (all of them being initialized by running a K-Means). Thus if EM generally perform better than K-Means, their timing difference probably explain why K-Means is still very popular.

We chosen to keep Starcraft dataset here to have an example of a case were both clustering and dimension reduction were not capable of producing exploitable data. This exploration is important, when you explore a new dataset, figuring out if you are more in the Iris case than Starcraft case quickly can save a lot of time.

As a comparison, we also tried to apply our network from homework 1 to Starcraft with a layer of only two neurones to force it to do a dimension reduction. On all the possibility tested, the best validation accuracy was lower than 7\%. This result is an other indication that lot of information are lost by reducing the dimension to two.

To improve this report we also tried to display Starcraft on three dimensions plots, but those plots were even less readable than the two dimensions one. In addition if human eyes can distinguish millions of colors, we did not fund a good way to make all the players perfectly distinguishable on the figures, while this is not crucial for our conclusion, we are sorry for the inconvenience.

We also apologies for the lack of figures in this section, but due to space constraint we have to do some sacrifice. Thus the plots of the different cluster were not included here as easy to imagine from the projections.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Training a Neural Network after dimension reduction}
\paragraph{}
Reducing dimension before trying to learn a classifier such as a neural network (NN) is a good solution to limit the effect of the curse of dimensions. One may expect that reducing the number of dimension will reduce the convergence time of a neural network, but this may not be true as compressing the information may make it harder for a NN to find useful way to use it.

To properly asses that question, among others, we will use here the same network structure from Homework 1. In this structure, we set most of the parameters explored in this previous work. For example, we used batch normalization in all our experiment, as well as the sigmoid activation function and the rmsprop optimizer from Keras. Thus, the reader can easily compare the results with the one presented previously.

We will also stick to the same graphical convention as previously: the hard line represent the median value while the hue area go from the minimum to the maximum.

The same convention on the description of the NN structure will also hold. $(n)$ describe a perceptron with one hidden layer of $n$ neurones and $(a,b)$ a perceptron of two layers with $a$ and $b$ neurones respectively.

\paragraph{}
If the part of the structure of the NN stay the same than in Homework 1, we still explored here a different set of parameters. The most important among them are number of features, the dimension reduction algorithm and the shape of the hidden layers. This last parameters cover the same range as in Homework 1, to keep comparable results. The dimension reduction was first fit on the train dataset and then applied to both train and test data. Repartition between train and test reminded the same than in assignment one, please refer to it for more information.

\paragraph{}
Before going more in depth, algorithm by algorithm, we will draw here some global trends.

As you can see on Figure \ref{fig:per_all_rm}, the different reduction methods does not gave the same results. However, we must note that, apart from the particular case of LDA, the median result of all the algorithm converge toward the same accuracy. It also worth mentioning that ICA gave some hard time to the NN with first improvements appearing far later than others methods.

It was also easy to predict that the more feature you give the better the result. Such a comportment can be seen on Figure \ref{fig:per_all_nf}, where you can see that there is a clear progression from 5 to 20 features but that the evolution is then far smaller. This is particularly truth for the maximum, which have nearly all the same values. Thus as a first impression we can say that 20 features is enough for a NN to learn how to classify the different players. We must also note that adding new feature does not augment the number of epoch needed by the NN to converge. This is probably because with more features it's easier for gradient descant to find one to focus on.

\begin{figure}
  \centering
  \begin{subfigure}[t]{0.49\columnwidth} 
    \centering 
    \includegraphics[width=1.1\linewidth]{{../plotter/graphs/all_epoch_val_categorical_accuracy_number_of_feature}.png}
    \caption{Effect of the number of feature}
    \label{fig:per_all_nf}
  \end{subfigure}
  \begin{subfigure}[t]{0.49\columnwidth} 
    \centering 
    \includegraphics[width=1.1\linewidth]{{../plotter/graphs/all_epoch_val_categorical_accuracy_reduction_method}.png}
    \caption{Effect of the reduction method used}
    \label{fig:per_all_rm}
  \end{subfigure}
  \caption{Validation accuracy according to the epoch for all the different combination}
  \label{fig:per_all} 
\end{figure}

\subsection{PCA}
\paragraph{}
PCA is the best performer here, after LDA. As you can see on Figure \ref{fig:per_pca_nf}, the global variation observed previously with the number features perfectly fit here. It is also hard not to notice the pick near epoch 400 for 40 features. By selecting only a subset of layers we can see that, as displayed on Figure \ref{fig:per_pca_nf_3525}, this pick is created by layers $(35)$ and $(25)$, that are able to take advantage of the extra features to improve their accuracy. The interesting fact here is that this advantage is quickly transformed into overfitting. Overfitting also occur for 35 features, with accuracy in that case never exceeding the one get with 30 features going as low as the one with only 10 features.

Adding new input features rise the number of parameters in the network, which also rise it chance to overfit. But the reason here is probably more prosaic, and can resumed by the curse of dimensions. The lack of examples here probably forbid the NN to properly integrate the new features letting it overfit on the train set.

\begin{figure}
  \centering
  \begin{subfigure}[t]{0.49\columnwidth} 
    \centering 
    \includegraphics[width=1.1\linewidth]{{../plotter/graphs/PCA_epoch_val_categorical_accuracy_number_of_feature}.png}
    \caption{Effect of the number of features for all tested layers}
    \label{fig:per_pca_nf}
  \end{subfigure}
  \begin{subfigure}[t]{0.49\columnwidth} 
    \centering 
    \includegraphics[width=1.1\linewidth]{{../plotter/graphs/PCA_epoch_val_categorical_accuracy_number_of_feature_35_25}.png}
    \caption{Effect of the number of features for $(35)$ and $(25)$ layers}
    \label{fig:per_pca_nf_3525}
  \end{subfigure}
  \caption{Validation accuracy according to the epoch with PCA}
  \label{fig:per_pca} 
\end{figure}

\subsection{ICA}
\paragraph{}
\subsection{Randomized projections}
\paragraph{}
\subsection{LDA}
\paragraph{}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Adding cluster as labels}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Conclusion}

\end{document}
